{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation\n",
        "### Sequence to Sequence Network with Attention\n"
      ],
      "metadata": {
        "id": "UXH7vNdyMViv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "\n",
        "# Device Agnostic Setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cx6DIyo2tZps",
        "outputId": "ac8587a1-f2fe-4eb5-f987-afdba0861c57"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 00. Loading Data Files"
      ],
      "metadata": {
        "id": "osTdaznQtsPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Language:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # For counting SOS and EOS token\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "rIiTXm0PujxQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase, rim and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    # Add spaces around punctuation\n",
        "    s = re.sub(r\"([.!?।])\", r\" \\1\", s)\n",
        "    # Retain Devanagari characters, Latin characters, English numbers, and punctuation\n",
        "    s = re.sub(r\"[^\\u0900-\\u097Fa-zA-Z0-9.!?']+\", r\" \", s)\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "4QhgZI522Cna"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def readLangs(file_path, lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and parse CSV content\n",
        "    with open(file_path, encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        lines = list(reader)\n",
        "\n",
        "    # Normalize each pair\n",
        "    pairs = [[normalizeString(s) for s in l] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Language(lang2)\n",
        "        output_lang = Language(lang1)\n",
        "    else:\n",
        "        input_lang = Language(lang1)\n",
        "        output_lang = Language(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "Gky103-BwBAG"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 24\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "aWeds-xmwfWI"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(file_path, lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(file_path, lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "Yriz4yBkwvFp"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng_nep_nmt.csv', 'eng', 'nep', False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjKhN9rOw6sq",
        "outputId": "4a1ba79d-80b6-487b-9ccb-023122f72bbe"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 160259 sentence pairs\n",
            "Trimmed to 119560 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 44799\n",
            "nep 119592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.choice(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY1y7N_Kz-uz",
        "outputId": "a05215a9-8b24-45f9-aa6a-9e2492eea24e"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tension started on monday from kathmandu on september 9 .',\n",
              " 'चैत २३ गते बेलुकीदेखि काठमाण्डौमा तनाव शुरु भएको थियो ।']"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    }
  ]
}